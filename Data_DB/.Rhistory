'really liked it',
'it was amazing')]
# Filtering by review length
data <- data[nchar(data$review) >= 5]
#Recoding the ratings
data$rating[data$rating == 'did not like it'] <- 1
data$rating[data$rating == 'it was ok'      ] <- 2
data$rating[data$rating == 'liked it'       ] <- 3
data$rating[data$rating == 'really liked it'] <- 4
data$rating[data$rating == 'it was amazing' ] <- 5
data$rating <- as.integer(data$rating)
# Removing the language and author variables and adding a review_id column
data$language <- NULL
data$reviewer <- NULL
data$review.id <- 1:nrow(data)
### Exploratory data analysis ###
#Looking at the distribution of ratings
barplot(table(as.factor(data$rating)),
ylim = c(0,5000),
main = "Distribution of ratings")
#Looking at the distribution of review lengths, and removing the longest reviews
data$review.length <- nchar(data$review)
hist(data$review.length,
ylim = c(0,5000),
main = "Distribution of review length" )
n <- nrow(data[data$review.length>=8000])
#Removing the reviews with more than 8000 characters
data <- data[data$review.length <= 8000]
hist(data$review.length,
ylim = c(0,3000),
main = "Distribution of review length" )
# Boxplot of review length by rating
with(data, boxplot(review.length~rating,
main = "Distribution of review length by rating"))
### Sentiment analysis with tidytext ###
# Loading the first sentiment score lexicon
AFINN <- sentiments %>%
filter(lexicon == "AFINN") %>%
select(word, afinn.score = score)
head(AFINN)
# Loading the second sentiment score lexicon
Bing <- sentiments %>%
filter(lexicon == "bing") %>%
select(word, bing.sentiment = sentiment)
head(Bing)
# "tidying" up the data (1 word per row) and adding the sentiment scores for each word
review.words <- data %>%
unnest_tokens(word, review) %>%
select(-c(book, review.length)) %>%
left_join(AFINN, by = "word") %>%
left_join(Bing, by = "word")
# Grouping by mean for observation
review.mean.sentiment <- review.words %>%
group_by(review.id, rating) %>%
summarize(mean.sentiment = mean(afinn.score, na.rm = TRUE))
# Plotting the result
theme_set(theme_bw())
ggplot(review.mean.sentiment, aes(rating, mean.sentiment, group = rating)) +
geom_boxplot() +
ylab("Average sentiment score")
# Transferring the results to our dataset
review.mean.sentiment <- review.mean.sentiment %>%
select(-rating) %>%
data.table()
clean.data <- data %>%
left_join(review.mean.sentiment, by = "review.id")
# Same as previous, but with the median
review.median.sentiment <- review.words %>%
group_by(review.id, rating) %>%
summarize(median.sentiment = median(afinn.score, na.rm = TRUE))
theme_set(theme_bw())
ggplot(review.median.sentiment, aes(rating, median.sentiment, group = rating)) +
geom_boxplot() +
ylab("Median sentiment score")
# Transferring the results to our dataset
review.median.sentiment <- review.median.sentiment %>%
select(-rating) %>%
data.table()
clean.data <- clean.data %>%
left_join(review.median.sentiment, by = "review.id")
# Counting the number of negative words per review according to AFINN lexicon
review.count.afinn.negative <- review.words %>%
filter(afinn.score < 0) %>%
group_by(review.id, rating) %>%
summarize(count.afinn.negative = n())
# Transferring the results to our dataset
review.count.afinn.negative <- review.count.afinn.negative %>%
select(-rating) %>%
data.table()
clean.data <- clean.data %>%
left_join(review.count.afinn.negative, by = "review.id")
# Counting the number of positive words per review according to AFINN lexicon
review.count.afinn.positive <- review.words %>%
filter(afinn.score > 0) %>%
group_by(review.id, rating) %>%
summarize(count.afinn.positive = n())
# Transferring the results to our dataset
review.count.afinn.positive <- review.count.afinn.positive %>%
select(-rating) %>%
data.table()
clean.data <- clean.data %>%
left_join(review.count.afinn.positive, by = "review.id")
# Counting the number of negative words per review according to Bing lexicon
review.count.bing.negative <- review.words %>%
filter(bing.sentiment == "negative") %>%
group_by(review.id, rating) %>%
summarize(count.bing.negative = n())
# Transferring the results to our dataset
review.count.bing.negative <- review.count.bing.negative %>%
select(-rating) %>%
data.table()
clean.data <- clean.data %>%
left_join(review.count.bing.negative, by = "review.id")
# Counting the number of positive words per review according to Bing lexicon
review.count.bing.positive <- review.words %>%
filter(bing.sentiment == "positive") %>%
group_by(review.id, rating) %>%
summarize(count.bing.positive = n())
# Transferring the results to our dataset
review.count.bing.positive <- review.count.bing.positive %>%
select(-rating) %>%
data.table()
clean.data <- clean.data %>%
left_join(review.count.bing.positive, by = "review.id")
# Writing the data to file for future analyses
write.csv(clean.data, "GoodReadsCleanData.csv", row.names = FALSE)
# Summarizing words by number of uses and average rating of review
word.mean.summaries <- review.words %>%
count(review.id, rating, word) %>%
group_by(word) %>%
summarize(reviews = n(),
uses = sum(n),
average.rating = mean(rating)) %>%
filter(reviews >= 3) %>%
arrange(average.rating)
# comparing AFINN score with average rating of reviews, per word
word.mean.afinn <- word.mean.summaries %>%
inner_join(AFINN)
ggplot(word.mean.afinn, aes(afinn.score, average.rating, group = afinn.score)) +
geom_boxplot() +
xlab("AFINN score of word") +
ylab("Mean rating of reviews with this word")
head(AFINN)
review.words <- data %>%
unnest_tokens(word, review) %>%
select(-c(book, review.length)) %>%
left_join(AFINN, by = "word") %>%
left_join(Bing, by = "word")
theme_set(theme_bw())
ggplot(review.mean.sentiment, aes(rating, mean.sentiment, group = rating)) +
geom_boxplot() +
ylab("Average sentiment score")
barplot(table(as.factor(data$rating)),
ylim = c(0,5000),
main = "Distribution of ratings")
review.mean.sentiment <- review.words %>%
group_by(review.id, rating) %>%
summarize(mean.sentiment = mean(afinn.score, na.rm = TRUE))
review.mean.sentiment <- review.mean.sentiment %>%
select(-rating) %>%
data.table()
clean.data <- data %>%
left_join(review.mean.sentiment, by = "review.id")
review.median.sentiment <- review.words %>%
group_by(review.id, rating) %>%
summarize(median.sentiment = median(afinn.score, na.rm = TRUE))
theme_set(theme_bw())
ggplot(review.median.sentiment, aes(rating, median.sentiment, group = rating)) +
geom_boxplot() +
ylab("Median sentiment score")
review.median.sentiment <- review.median.sentiment %>%
select(-rating) %>%
data.table()
clean.data <- clean.data %>%
left_join(review.median.sentiment, by = "review.id")
review.count.afinn.negative <- review.words %>%
filter(afinn.score < 0) %>%
group_by(review.id, rating) %>%
summarize(count.afinn.negative = n())
# Transferring the results to our dataset
review.count.afinn.negative <- review.count.afinn.negative %>%
select(-rating) %>%
data.table()
clean.data <- clean.data %>%
left_join(review.count.afinn.negative, by = "review.id")
library(data.table)
library(dplyr)
library(caret)
library(RTextTools)
library(xgboost)
library(ROCR)
install.packages("xgboost")
library(ROCR)
data <- read.csv("/Users/bekterra/Desktop/R - Project scripts/GoodReadsCleanData.csv", stringsAsFactors = FALSE)
set.seed(1234)
data$good.read <- 0
data$good.read[data$rating == 4 | data$rating == 5] <- 1
trainIdx <- createDataPartition(data$good.read,
p = .75,
list = FALSE,
times = 1)
train <- data[trainIdx, ]
test <- data.table(data[-trainIdx, ])
sparsity <- .99
bad.dtm <- create_matrix(train$review[train$good.read == 0],
language = "english",
removeStopwords = FALSE,
removeNumbers = TRUE,
stemWords = FALSE,
removeSparseTerms = sparsity)
#Converting the DTM in a data frame
bad.dtm.df  <-  data.table(as.matrix(bad.dtm))
# Creating a DTM for the positive reviews
good.dtm <- create_matrix(train$review[train$good.read == 1],
language="english",
removeStopwords = FALSE,
removeNumbers = TRUE,
stemWords = FALSE,
removeSparseTerms = sparsity)
good.dtm.df <- data.table(as.matrix(good.dtm))
# Joining the two DTM together
train.dtm.df <- bind_rows(bad.dtm.df, good.dtm.df)
train.dtm.df$review.id <- c(train$review.id[train$good.read == 0],
train$review.id[train$good.read == 1])
train.dtm.df <- arrange(train.dtm.df, review.id)
train.dtm.df$good.read  <- train$good.read
train.dtm.df <- train %>%
select(-c(book, rating, review, good.read)) %>%
inner_join(train.dtm.df, by = "review.id") %>%
select(-review.id)
train.dtm.df[is.na(train.dtm.df)] <- 0
# Creating the test DTM
test.dtm <- create_matrix(test$review,
language = "english",
removeStopwords = FALSE,
removeNumbers = TRUE,
stemWords = FALSE,
removeSparseTerms = sparsity)
test.dtm.df <- data.table(as.matrix(test.dtm))
test.dtm.df$review.id <- test$review.id
test.dtm.df$good.read <- test$good.read
test.dtm.df <- test %>%
select(-c(book, rating, review, good.read)) %>%
inner_join(test.dtm.df, by = "review.id") %>%
select(-review.id)
test.dtm.df <- head(bind_rows(test.dtm.df, train.dtm.df[1, ]), -1)
test.dtm.df <- test.dtm.df %>%
select(one_of(colnames(train.dtm.df)))
test.dtm.df[is.na(test.dtm.df)] <- 0
test.dtm.df <- data.table(test.dtm.df)
baseline.acc <- sum(test$good.read == "1") / nrow(test)
XGB.train <-  as.matrix(select(train.dtm.df, -good.read),
dimnames = dimnames(train.dtm.df))
XGB.test <- as.matrix(select(test.dtm.df, -good.read),
dimnames = dimnames(test.dtm.df))
XGB.model <- xgboost(data = XGB.train,
label = train.dtm.df$good.read,
nrounds = 400,
objective = "binary:logistic")
library(xgboost)
baseline.acc <- sum(test$good.read == "1") / nrow(test)
XGB.train <-  as.matrix(select(train.dtm.df, -good.read),
dimnames = dimnames(train.dtm.df))
XGB.test <- as.matrix(select(test.dtm.df, -good.read),
dimnames = dimnames(test.dtm.df))
XGB.model <- xgboost(data = XGB.train,
label = train.dtm.df$good.read,
nrounds = 400,
objective = "binary:logistic")
XGB.predict <- predict(XGB.model, XGB.test)
XGB.results <- data.frame(good.read = test$good.read, pred = XGB.predict)
ROCR.pred <- prediction(XGB.results$pred, XGB.results$good.read)
ROCR.perf <- performance(ROCR.pred, 'tnr','fnr')
plot(ROCR.perf, colorize = TRUE)
XGB.table  <- table(true = XGB.results$good.read,
pred = as.integer(XGB.results$pred >= 0.80))
XGB.table
XGB.acc <- sum(diag(XGB.table)) / nrow(test)
### Feature analysis with XGBoost
names <- colnames(test.dtm.df)
importance.matrix <- xgb.importance(names, model = XGB.model)
xgb.plot.importance(importance.matrix[1:20, ])
install.packages(c('repr', 'IRdisplay', 'crayon', 'pbdZMQ', 'devtools'))
devtools::install_github('IRkernel/IRkernel')
IRkernel::installspec()
IRkernel::installspec(user = FALSE)
library("IRkernel", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
IRkernel::installspec(user = FALSE)
summary(MergedFlights.df)
MergedFlights.df <-
do.call(rbind,
lapply(list.files(path = "/Users/bekterra/Desktop/Hackatons/Flight Delay/flightsDB/"), read.csv))
flightsDB <- fread('/Users/bekterra/Desktop/Hackatons/Flight Delay/CombinedFlights.csv',
header = TRUE, stringsAsFactors = FALSE)
library(data.table)
flightsDB <- fread('/Users/bekterra/Desktop/Hackatons/Flight Delay/CombinedFlights.csv',
header = TRUE, stringsAsFactors = FALSE)
flightsDB <- read.csv('/Users/bekterra/Desktop/Hackatons/Flight Delay/CombinedFlights.csv',
header = TRUE, stringsAsFactors = FALSE)
flightsDB <- read.csv("/Users/bekterra/Desktop/Hackatons/Flight Delay/CombinedFlights.csv",
header = TRUE, stringsAsFactors = FALSE)
GrouperFunc <- function(df, ...) df %>% group_by(list(...))
AirPlot("New York", "Chicago", "UNIQUE_CARRIER")
setwd("/Users/bekterra/Desktop/Hackatons/Flight Delay/flightsDB")
library(data.table)
# merging datasets into one dataframe
MergedFlights.df <-
do.call(rbind,
lapply(list.files(path = "/Users/bekterra/Desktop/Hackatons/Flight Delay/flightsDB/"), read.csv))
# Missing values
summary(flightsDB$ARR_DELAY)
head(flightsDB)
airportsDB <- read.csv('/Users/bekterra/Desktop/Hackatons/Flight Delay/Airport_Lookup.csv',
header = TRUE, stringsAsFactors = FALSE)
carriersDB <- read.csv('/Users/bekterra/Desktop/Hackatons/Flight Delay/Carrier_Lookup.csv',
header = TRUE, stringsAsFactors = FALSE)
flightsDB <- read.csv("/Users/bekterra/Desktop/Hackatons/Flight Delay/flightsDB/CombinedFlights.csv",
header = TRUE, stringsAsFactors = FALSE)
head(flightsDB)
flightsDB <- subset(flightsDB, select = -c(X, YEAR,V1,DEST_AIRPORT_SEQ_ID))
# Missing values
summary(flightsDB$ARR_DELAY)
dim(flightsDB)
# Creating a New Feature: Number of Days to Holiday
holidays <- c('2016-01-01', '2016-01-20', '2016-02-17', '2016-05-26',
'2016-07-04', '2016-09-01', '2016-10-13', '2016-11-11',
'2016-11-28', '2015-12-25') # Ten major holidays, including Memorial Day, Columbus Day, Labor Day, MLK Day
# Veteran's Day, New Year's, President's Day, Independence Day, Thanksgiving,
# and Christmas Day.
holidayDates <- as.Date(holidays)
DaysToHoliday <- function(month, day)
{ # Input a month and day from the flightsDB
# Get our year.
year <- 2016
if (month > 11)
{
year <- 2015
}
# Paste on a 2013 for November and December dates.
currDate <- as.Date(paste(year,month,day,sep = '-')) # Create a DATE object we can use to calculate the time difference
numDays <- as.numeric(min(abs(currDate-holidayDates))) # Now find the minimum difference between the date and our holidays
return(numDays)                                        # We can vectorize this to automatically find the minimum closest
# holiday by subtracting all holidays at once
}
datesOfYear <- unique(flightsDB[,1:2]) # Get all of the dates through unique Month/Day combinations
datesOfYear$HDAYS <- mapply(DaysToHoliday, datesOfYear$MONTH, datesOfYear$DAY_OF_MONTH)
InputDays <- function(month,day){
finalDays <- datesOfYear$HDAYS[datesOfYear$MONTH == month & datesOfYear$DAY_OF_MONTH == day] # Find which row to get
return(finalDays)
}
flightsDB$HDAYS <- mapply(InputDays, flightsDB$MONTH, flightsDB$DAY_OF_MONTH)
setwd("/Users/bekterra/Desktop/Hackatons/Flight Delay/flightsDB")
library(data.table)
# merging datasets into one dataframe
MergedFlights.df <-
do.call(rbind,
lapply(list.files(path = "/Users/bekterra/Desktop/Hackatons/Flight Delay/flightsDB/"), read.csv))
summary(MergedFlights.df)
#Saving combined dataframe
output.filename <- "CombinedFlights.csv"
write.csv(MergedFlights.df, output.filename)
# loading datasets
airportsDB <- read.csv('/Users/bekterra/Desktop/Hackatons/Flight Delay/Airport_Lookup.csv',
header = TRUE, stringsAsFactors = FALSE)
carriersDB <- read.csv('/Users/bekterra/Desktop/Hackatons/Flight Delay/Carrier_Lookup.csv',
header = TRUE, stringsAsFactors = FALSE)
flightsDB <- read.csv("/Users/bekterra/Desktop/Hackatons/Flight Delay/flightsDB/CombinedFlights.csv",
header = TRUE, stringsAsFactors = FALSE)
head(flightsDB)
flightsDB <- subset(flightsDB, select = -c(X, YEAR,V1,DEST_AIRPORT_SEQ_ID))
flightsDB <- subset(flightsDB, select = -c(X, YEAR,X.1,DEST_AIRPORT_SEQ_ID))
summary(flightsDB$ARR_DELAY)
dim(flightsDB)
flightsDB <- na.omit(flightsDB)
summary(flightsDB)
# Creating a New Feature: Number of Days to Holiday
holidays <- c('2016-01-01', '2016-01-20', '2016-02-17', '2016-05-26',
'2016-07-04', '2016-09-01', '2016-10-13', '2016-11-11',
'2016-11-28', '2015-12-25') # Ten major holidays, including Memorial Day, Columbus Day, Labor Day, MLK Day
# Veteran's Day, New Year's, President's Day, Independence Day, Thanksgiving,
# and Christmas Day.
holidayDates <- as.Date(holidays)
DaysToHoliday <- function(month, day)
{ # Input a month and day from the flightsDB
# Get our year.
year <- 2016
if (month > 11)
{
year <- 2015
}
# Paste on a 2013 for November and December dates.
currDate <- as.Date(paste(year,month,day,sep = '-')) # Create a DATE object we can use to calculate the time difference
numDays <- as.numeric(min(abs(currDate-holidayDates))) # Now find the minimum difference between the date and our holidays
return(numDays)                                        # We can vectorize this to automatically find the minimum closest
# holiday by subtracting all holidays at once
}
datesOfYear <- unique(flightsDB[,1:2]) # Get all of the dates through unique Month/Day combinations
datesOfYear$HDAYS <- mapply(DaysToHoliday, datesOfYear$MONTH, datesOfYear$DAY_OF_MONTH)
# Apply our function in a vectorized manner via one of R's many "apply" functions (in this case mapply)
# to each unique date and save
head(datesOfYear)
InputDays <- function(month,day){
finalDays <- datesOfYear$HDAYS[datesOfYear$MONTH == month & datesOfYear$DAY_OF_MONTH == day] # Find which row to get
return(finalDays)
}
flightsDB$HDAYS <- mapply(InputDays, flightsDB$MONTH, flightsDB$DAY_OF_MONTH)
head(flightsDB)
flightsDB$ARR_HOUR <- trunc(flightsDB$CRS_ARR_TIME/100) # Cuts off the minutes, essentially.
flightsDB$DEP_HOUR <- trunc(flightsDB$CRS_DEP_TIME/100)
library(dplyr)
head(airportsDB)
subset(airportsDB, grepl('New York', Description))
MaxFlightsCode <- function(code){
# Calculates the most likely flights based on flight frequency.
codeFrame <- subset(flightsDB, ORIGIN_AIRPORT_ID == code)
numFlights <- dim(codeFrame)[1]
# Get the number of rows in this frame
return(numFlights)
}
AirportCode <- function(city){
codes <- subset(airportsDB, grepl(city, Description))
# Find all of the airports matching the city we entered
codes$NumFlights <- sapply(codes$Code, MaxFlightsCode)
# Collect how many flights exist in the database starting at this airport
codes <- subset(codes, NumFlights == max(NumFlights))$Code
# Return our top airport
return(codes)
}
AirportCode('New York')
GrouperFunc <- function(df, ...) df %>% group_by(list(...))
library(ggplot2)
AirPlot <- function(departure, arrival, groupon){
# Departure and arrival can be cities that are being entered.
departCode <- AirportCode(departure)
arriveCode <- AirportCode(arrival) # Call our earlier AirportCode function to get the airport ID
tempDB <- subset(flightsDB, ORIGIN_AIRPORT_ID == departCode & DEST_AIRPORT_ID == arriveCode) # Only get flights for our
grouped <- GrouperFunc(tempDB, groupon)                                                       # flight path
# Use our GrouperFunc to have dplyr group our data into categories
summaryDF <- summarize(grouped, mean = mean(ARR_DELAY)) # Call summarize from our grouped data frame
# Now that the data is in a good format, create the ggplot bar chart
finalBarPlot <- ggplot(summaryDF, aes_string(x=groupon, y='mean')) +
geom_bar(color="black", width = 0.2, stat = 'identity') +
guides(fill=FALSE)+
xlab(groupon) +
ylab('Average Delay (minutes)')+
ggtitle((paste('Flights from', departure, 'to', arrival)))
return(finalBarPlot)
}
AirPlot("New York", "Chicago", "UNIQUE_CARRIER")
GrouperFunc <- function(df, ...) df %>% regroup(list(...))
library(ggplot2)
AirPlot <- function(departure, arrival, groupon){
# Departure and arrival can be cities that are being entered.
departCode <- AirportCode(departure)
arriveCode <- AirportCode(arrival) # Call our earlier AirportCode function to get the airport ID
tempDB <- subset(flightsDB, ORIGIN_AIRPORT_ID == departCode & DEST_AIRPORT_ID == arriveCode) # Only get flights for our
grouped <- GrouperFunc(tempDB, groupon)                                                       # flight path
# Use our GrouperFunc to have dplyr group our data into categories
summaryDF <- summarize(grouped, mean = mean(ARR_DELAY)) # Call summarize from our grouped data frame
# Now that the data is in a good format, create the ggplot bar chart
finalBarPlot <- ggplot(summaryDF, aes_string(x=groupon, y='mean')) +
geom_bar(color="black", width = 0.2, stat = 'identity') +
guides(fill=FALSE)+
xlab(groupon) +
ylab('Average Delay (minutes)')+
ggtitle((paste('Flights from', departure, 'to', arrival)))
return(finalBarPlot)
}
AirPlot("New York", "Chicago", "UNIQUE_CARRIER")
GrouperFunc <- function(df, ...) df %>% group_by_(list(...))
library(ggplot2)
AirPlot <- function(departure, arrival, groupon){
# Departure and arrival can be cities that are being entered.
departCode <- AirportCode(departure)
arriveCode <- AirportCode(arrival) # Call our earlier AirportCode function to get the airport ID
tempDB <- subset(flightsDB, ORIGIN_AIRPORT_ID == departCode & DEST_AIRPORT_ID == arriveCode) # Only get flights for our
grouped <- GrouperFunc(tempDB, groupon)                                                       # flight path
# Use our GrouperFunc to have dplyr group our data into categories
summaryDF <- summarize(grouped, mean = mean(ARR_DELAY)) # Call summarize from our grouped data frame
# Now that the data is in a good format, create the ggplot bar chart
finalBarPlot <- ggplot(summaryDF, aes_string(x=groupon, y='mean')) +
geom_bar(color="black", width = 0.2, stat = 'identity') +
guides(fill=FALSE)+
xlab(groupon) +
ylab('Average Delay (minutes)')+
ggtitle((paste('Flights from', departure, 'to', arrival)))
return(finalBarPlot)
}
AirPlot("New York", "Chicago", "UNIQUE_CARRIER")
GrouperFunc <- function(df, ...) df %>% group_by_(.dots =...)
AirportCode('New York')
library(ggplot2)
AirPlot <- function(departure, arrival, groupon){
# Departure and arrival can be cities that are being entered.
departCode <- AirportCode(departure)
arriveCode <- AirportCode(arrival) # Call our earlier AirportCode function to get the airport ID
tempDB <- subset(flightsDB, ORIGIN_AIRPORT_ID == departCode & DEST_AIRPORT_ID == arriveCode) # Only get flights for our
grouped <- GrouperFunc(tempDB, groupon)                                                       # flight path
# Use our GrouperFunc to have dplyr group our data into categories
summaryDF <- summarize(grouped, mean = mean(ARR_DELAY)) # Call summarize from our grouped data frame
# Now that the data is in a good format, create the ggplot bar chart
finalBarPlot <- ggplot(summaryDF, aes_string(x=groupon, y='mean')) +
geom_bar(color="black", width = 0.2, stat = 'identity') +
guides(fill=FALSE)+
xlab(groupon) +
ylab('Average Delay (minutes)')+
ggtitle((paste('Flights from', departure, 'to', arrival)))
return(finalBarPlot)
}
AirPlot("New York", "Chicago", "UNIQUE_CARRIER")
subset(carriersDB, grepl('^AA$|^B6$|^DL$', Code))
AirPlot('New York', 'Chicago', 'MONTH')
